# Enterprise AI Brain (RAG Infrastructure)

[ **English** ] | [ [ä¸­æ–‡](./README_zh.md) ]

> A production-grade AI Knowledge Base backend featuring **Dual-Layer RAG**, **Async Inference**, and **Persistent Memory**. Built for scale with high-performance engineering standards.

![Architecture](https://img.shields.io/badge/Architecture-Microservices-blue) ![Python](https://img.shields.io/badge/Python-3.10%2B-green) ![License](https://img.shields.io/badge/License-MIT-purple)

## ðŸ— System Architecture

This project implements a scalable **Retrieval-Augmented Generation (RAG)** pipeline designed to solve common enterprise challenges: Data Hallucination, Context Window Limits, and Stateless Interactions.

```mermaid
graph TD
    Client[Client / Frontend] -->|SSE Stream| Gateway[FastAPI Gateway]
    Gateway -->|Async| Service[Chat Service]
    
    subgraph "Data Plane"
        Service -->|Retrieve| Milvus[(Milvus Vector DB)]
        Service -->|Read/Write| Redis[(Redis Memory Store)]
    end
    
    subgraph "Compute Plane"
        Service -->|Inference| vLLM[vLLM Inference Server]
        vLLM -->|GPU| GPU[NVIDIA GPU]
    end

```

## ðŸš€ Key Features

* **High-Performance Inference:** Decoupled inference engine using **vLLM** (PagedAttention) for 10x throughput compared to naive Transformers.
* **Enterprise RAG:**
* **Milvus** for billion-scale vector storage.
* **Hybrid Search** (Semantic + Keyword) support.
* **LlamaIndex** orchestration for advanced chunking strategies.


* **Contextual Memory:** **Redis-backed session management** allows the AI to remember user context across multi-turn conversations (Sliding Window strategy).
* **Streaming First:** Full **Server-Sent Events (SSE)** support for <500ms Time-To-First-Token (TTFT).
* **Production Ready:**
* **Requirements.txt** for dependency management.
* **Dockerized** for consistent deployment.
* **Clean Architecture** (Controller-Service-Repository pattern).



## ðŸ›  Tech Stack

* **Framework:** FastAPI, Uvicorn
* **Orchestration:** LlamaIndex (Advanced RAG)
* **Vector Database:** Milvus (or Milvus Lite for dev)
* **Inference:** vLLM (OpenAI Compatible API)
* **Embedding:** BAAI/bge-m3 (SOTA Chinese/English Embedding)
* **Memory:** Redis
* **Containerization:** Docker, Docker Compose

## âš¡ï¸ Quick Start

### Prerequisites

* Docker & Docker Compose
* Python 3.10+ (if running locally)
* Access to a GPU server (for vLLM) or use OpenAI API key

### 1. Setup Environment

```bash
# Clone the repo
git clone [https://github.com/your-username/enterprise-ai-backend.git](https://github.com/your-username/enterprise-ai-backend.git)
cd enterprise-ai-backend

# Install dependencies (if running locally)
pip install -r requirements.txt

# For development, install dev dependencies
pip install -r requirements-dev.txt

# Configure Environment Variables
cp .env.example .env
# Edit .env to point to your vLLM/Milvus/Redis instances

```

### 2. Run with Docker Compose

```bash
docker-compose up -d --build

```

The API will be available at `http://localhost:8080`.
Swagger UI: `http://localhost:8080/docs`

### 3. Usage Examples

**Ingest a Document:**

```bash
curl -X POST "http://localhost:8080/api/v1/ingest/upload" \
     -H "accept: application/json" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@./technical_spec.pdf"

```

**Chat with Streaming (SSE):**

```bash
curl -N -X POST "http://localhost:8080/api/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -d '{
           "query": "What is the architecture of this system?",
           "session_id": "user_123",
           "stream": true
         }'

```

## ðŸ“‚ Project Structure

```text
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ api          # API Controllers (FastAPI Routers)
â”‚   â”‚   â””â”€â”€ v1       # API v1 endpoints (chat, ingest, admin, llm, vector-store, memory)
â”‚   â”œâ”€â”€ core         # Global Configs & Security
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”‚   â”œâ”€â”€ config_validator.py # Configuration validation
â”‚   â”‚   â”œâ”€â”€ security.py        # API key validation
â”‚   â”‚   â”œâ”€â”€ exceptions.py      # Exception handlers
â”‚   â”‚   â”œâ”€â”€ logging.py         # Logging setup
â”‚   â”‚   â”œâ”€â”€ middleware.py      # Request middleware
â”‚   â”‚   â”œâ”€â”€ retry.py           # Retry mechanism
â”‚   â”‚   â”œâ”€â”€ connections.py     # Connection pools
â”‚   â”‚   â””â”€â”€ cache.py           # Caching layer
â”‚   â”œâ”€â”€ models       # Pydantic Schemas
â”‚   â”œâ”€â”€ services     # Business Logic
â”‚   â”‚   â”œâ”€â”€ chat_service.py    # RAG chat service
â”‚   â”‚   â”œâ”€â”€ ingest_service.py  # Document ingestion
â”‚   â”‚   â””â”€â”€ memory_service.py  # Memory management
â”‚   â””â”€â”€ utils        # Factories & Utilities
â”‚       â”œâ”€â”€ llm_factory.py           # LLM initialization
â”‚       â”œâ”€â”€ llm_providers.py         # Multi-LLM support
â”‚       â”œâ”€â”€ vector_store_providers.py # Multi-vector store support
â”‚       â”œâ”€â”€ memory_providers.py      # Multi-memory support
â”‚       â”œâ”€â”€ chunking.py              # Document chunking
â”‚       â””â”€â”€ vector_store.py          # Vector store manager
â”œâ”€â”€ tests            # Pytest Suites
â”œâ”€â”€ Dockerfile       # Multi-stage build
â”œâ”€â”€ docker-compose.yml # Docker orchestration
â”œâ”€â”€ requirements.txt # Production dependencies
â”œâ”€â”€ requirements-dev.txt # Development dependencies
â”œâ”€â”€ .env.example    # Environment variables template
â””â”€â”€ README.md        # Project documentation
```

## ðŸ“š Documentation

- [OPTIMIZATION_SUMMARY.md](OPTIMIZATION_SUMMARY.md) - ä¼˜åŒ–æ€»ç»“
- [MULTI_LLM_SUPPORT.md](MULTI_LLM_SUPPORT.md) - å¤šLLMæ”¯æŒæ–‡æ¡£
- [MULTI_VECTOR_STORE_SUPPORT.md](MULTI_VECTOR_STORE_SUPPORT.md) - å¤šå‘é‡æ•°æ®åº“æ”¯æŒæ–‡æ¡£
- [MULTI_MEMORY_SUPPORT.md](MULTI_MEMORY_SUPPORT.md) - å¤šè®°å¿†æ–¹æ¡ˆæ”¯æŒæ–‡æ¡£
- [PROJECT_REVIEW.md](PROJECT_REVIEW.md) - é¡¹ç›®å®¡æŸ¥æŠ¥å‘Š
- [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) - éƒ¨ç½²æŒ‡å—
- [TROUBLESHOOTING.md](TROUBLESHOOTING.md) - æ•…éšœæŽ’æŸ¥æŒ‡å—
- [tests/README.md](tests/README.md) - æµ‹è¯•æ–‡æ¡£

```

---

*Built with â¤ï¸ by zhongshoujin. Open for AI Backend opportunities.*

```